{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# EDA and Modeling Classes for Machine Learning and Datascience Projects\n",
    "\n",
    "In this code i developed a method that helped me to do EDA faster than do it step by step and reduce your line codes.\n",
    "You can do classes for repetitive processes for example: Regression, Classification models, Logistic regression, among others i hope this notebook will help you to do more efficient coding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries for the project"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq as gbq\n",
    "import db_dtypes\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "#plt.style.use('fivethirtyeight')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from scipy.stats import zscore\n",
    "from sklearn.cluster import KMeans\n",
    "from termcolor import colored as cl # text customization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Outlier detection and treatment class\n",
    "This class works as follows:\n",
    "1. First we find the outliers of our dataset using IQR Method, it is only to explore our data\n",
    "2. Next we drop the outliers with differents methos like Drop, Cap, Replace"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OutliersStats:\n",
    "    def __init__(self, dataframe_process):\n",
    "        self.dataframe_process = dataframe_process\n",
    "\n",
    "    def find_outliers_iqr(self):\n",
    "        q1 = self.dataframe_process.quantile(0.25)\n",
    "        q3 = self.dataframe_process.quantile(0.75)\n",
    "        IQR = q3 - q1\n",
    "        outliers = self.dataframe_process[\n",
    "            ((self.dataframe_process < (q1 - 1.5 * IQR)) | (self.dataframe_process > (q3 + 1.5 * IQR)))]\n",
    "        print(\"number of outliers: \" + str(len(outliers)))\n",
    "        print(\"max outlier value: \" + str(outliers.max()))\n",
    "        print(\"min outlier value: \" + str(outliers.min()))\n",
    "        return outliers\n",
    "\n",
    "    def outliers_drop_method(self):\n",
    "        q1 = self.dataframe_process.quantile(0.25)\n",
    "        q3 = self.dataframe_process.quantile(0.75)\n",
    "        IQR = q3 - q1\n",
    "        not_outliers = self.dataframe_process[\n",
    "            ~((self.dataframe_process < (q1 - 1.5 * IQR)) | (self.dataframe_process > (q3 + 1.5 * IQR)))]\n",
    "        outliers_dropped = not_outliers.dropna().reset_index()\n",
    "        return outliers_dropped\n",
    "\n",
    "    def outliers_cap_method(self, column_outlier):\n",
    "        upper_limit = self.dataframe_process[column_outlier].mean() + 3 * self.dataframe_process[column_outlier].std()\n",
    "        lower_limit = self.dataframe_process[column_outlier].mean() - 3 * self.dataframe_process[column_outlier].std()\n",
    "        print(\"Upper limit: \" + str(upper_limit))\n",
    "        print(\"Lower limit: \" + str(lower_limit))\n",
    "        self.dataframe_process[column_outlier] = np.where(self.dataframe_process[column_outlier] > upper_limit,\n",
    "                                             upper_limit,\n",
    "                                             np.where(\n",
    "                                                 self.dataframe_process[column_outlier] < lower_limit,\n",
    "                                                 lower_limit,\n",
    "                                                 self.dataframe_process[column_outlier]\n",
    "                                             )\n",
    "                                             )\n",
    "        return self.dataframe_process[column_outlier]\n",
    "\n",
    "    def outliers_replace_method(self):\n",
    "        q1 = self.dataframe_process.quantile(0.25)\n",
    "        q3 = self.dataframe_process.quantile(0.75)\n",
    "        IQR = q3 - q1\n",
    "        upper = self.dataframe_process[~(self.dataframe_process > (q3 + 1.5 * IQR))].max()\n",
    "        lower = self.dataframe_process[~(self.dataframe_process < (q1 - 1.5 * IQR))].min()\n",
    "        df = np.where(self.dataframe_process > upper,\n",
    "                      self.dataframe_process.mean(),\n",
    "                      np.where(\n",
    "                          self.dataframe_process < lower,\n",
    "                          self.dataframe_process.mean(),\n",
    "                          self.dataframe_process\n",
    "                      )\n",
    "                      )\n",
    "        return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read from Google Cloud Platform\n",
    "This class allows you to load data from databases in Google Cloud Platform by enter your query and the client of your account with the json of your project"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ReadFromGCP:\n",
    "    def __init__(self, query, client):\n",
    "        self.query = query\n",
    "        self.client = client\n",
    "\n",
    "    def data_read(self):\n",
    "        data = self.client.query(self.query, location=\"US\", ).result().to_dataframe(progress_bar_type='tqdm')\n",
    "        return data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class for EDA Analysis\n",
    "\n",
    "Now in this class we use a lot of functions that allows us to do some of the classic analysis that usually are used in EDA\n",
    "1. Dimension: get the dimension of the dataframe in rows and columns\n",
    "2. Mean: get the mean of the dataset in a specific column\n",
    "3. Standard Deviation: get the sd of the dataset in a specific column\n",
    "4. Missing Values Barplot: The barplot provides a simple plot where each bar represents a column within the dataframe. The height of the bar indicates how complete that column is, i.e, how many non-null values are present\n",
    "5. Group Mean: we can calculate the mean of specific grouped column like a pivot table in order to get some metrics faster\n",
    "6. Proportion: proportion of the data in a column\n",
    "\n",
    "And other interesting functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EDA:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def get_dim(self):\n",
    "        print('Filas:', len(self.data))\n",
    "        print('Columnas:', len(list(self.data.columns)))\n",
    "\n",
    "    def get_mean(self, column):\n",
    "        print(f\"Mean {column}:\", self.data[column].mean())\n",
    "\n",
    "    def get_standard_dev(self, column):\n",
    "        print(f\"STD {column}:\", self.data[column].std())\n",
    "\n",
    "    def missing_values_bar(self):\n",
    "        msno.bar(self.data)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def group_mean_metric(self, column, valor):\n",
    "        metrica_mean = pd.DataFrame(self.data.groupby(column)[valor].mean()).reset_index()\n",
    "        metrica_mean.rename(columns={valor: 'Promedio del ' + valor}, inplace=True)\n",
    "        metrica_media = pd.DataFrame(self.data.groupby(column)[valor].median()).reset_index()\n",
    "        metrica_media.rename(columns={valor: 'Media del ' + valor}, inplace=True)\n",
    "        final_data = metrica_mean.merge(metrica_media, on=column, how='left')\n",
    "        return final_data\n",
    "\n",
    "    def proporcion_variables(self, column):\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        self.data[column].value_counts(normalize=True).plot.barh(width=0.4)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.title(\"Proporción de ventas por categoría\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_barplot_agrupado(self, column_categorica, value, funcion_agrupadora):\n",
    "        grupos = self.data.groupby(by=[column_categorica]).aggregate(\n",
    "            {value: funcion_agrupadora})\n",
    "        grupos2 = grupos.reset_index()\n",
    "        plt.bar(grupos2[column_categorica], grupos2[value])\n",
    "        plt.xticks(rotation=30, ha='right')\n",
    "        plt.xticks(fontsize=7)\n",
    "        plt.ylabel(value, fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.title(\"Variable categórica: \" + column_categorica +\n",
    "                  \" y Métrica: \" + value, fontsize=9)\n",
    "        plt.suptitle(\"Análisis a nivel de \" + column_categorica, fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        plt.style.use('seaborn-colorblind')\n",
    "        # plt.margins(0.5)\n",
    "        plt.show()\n",
    "\n",
    "    def get_pairplot(self, column_categorica, value, indice):\n",
    "        analisis_df = pd.pivot_table(self.data, values=value, index=[indice],\n",
    "                                     columns=[column_categorica], aggfunc=np.sum).reset_index()\n",
    "        sns.set()\n",
    "        pairplot = sns.pairplot(analisis_df)\n",
    "        plt.title(f\"{column_categorica} PairPlot\")\n",
    "        plt.show()\n",
    "\n",
    "    def get_heatmap_corr(self, column_categorica, value, indice):\n",
    "        analisis_df = pd.pivot_table(self.data, values=value, index=[indice],\n",
    "                                     columns=[column_categorica], aggfunc=np.sum).reset_index()\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        correlaciones = analisis_df.corr()\n",
    "        heatmap_corr = sns.heatmap(correlaciones, vmin=-1, vmax=1, annot=True)\n",
    "        heatmap_corr.set_title('Análisis de correlaciones', fontdict={'fontsize': 12}, pad=12)\n",
    "        plt.show()\n",
    "\n",
    "    def get_heatmap_improve(self, column_categorica, value, indice):\n",
    "        analisis_df = pd.pivot_table(self.data, values=value, index=[indice],\n",
    "                                     columns=[column_categorica], aggfunc=np.sum).reset_index()\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        correlaciones = analisis_df.corr()\n",
    "        heatmap_corr = sns.heatmap(correlaciones, vmin=-1, vmax=1, annot=True)\n",
    "        heatmap_corr.set_title('Análisis de correlaciones', fontdict={'fontsize': 12}, pad=12)\n",
    "        plt.show()\n",
    "\n",
    "    def get_heatmap_corr_v2(self):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(self.data.corr(),\n",
    "                    annot=True,\n",
    "                    linewidths=.5,\n",
    "                    center=0,\n",
    "                    cbar=False,\n",
    "                    cmap=\"YlGnBu\")\n",
    "        plt.title('Mapa de calor - Correlación variables')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_boxplot_horizontal(self, column_categorica, value):\n",
    "        p = sns.boxplot(y=self.data[column_categorica], x=self.data[value])\n",
    "        p.set_xlabel(value, fontsize=10)\n",
    "        p.set_ylabel(column_categorica, fontsize=10)\n",
    "        p.set_title(value + ' Distribution by ' + column_categorica, fontsize=12, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_boxplot_vertical(self, column_categorica, value):\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        sns.boxplot(x=self.data[column_categorica], y=self.data[value], showmeans=True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def transformacion_base(self, column_categorica, value, indice):\n",
    "        analisis_df = pd.pivot_table(self.data, values=value, index=[indice],\n",
    "                                     columns=[column_categorica], aggfunc=np.sum)\n",
    "        return analisis_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}